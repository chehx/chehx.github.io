<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Haoxuan Che (è½¦æ˜Šè½©), Principal Researcher at AI Lab, Huawei Hong Kong. Research: world model, multimodal generative models, AI agents. HKUST PhD.">
  <title>Haoxuan Che, Ph.D.</title>
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="css/style.css">
</head>

<body>

  <div class="container">
    
    <!-- LEFT SIDEBAR -->
    <aside class="sidebar">
      <div class="profile-card">
        <img src="images/misc/mainfig.png" alt="Haoxuan Che" class="avatar">
        <h1 class="name">Haoxuan Cheï¼ˆè½¦æ˜Šè½©ï¼‰</h1>
        <div class="role">Principal Researcher @ Huawei HKRC</div>
        
        <div class="social-links">
          <a href="mailto:hche@connect.ust.hk" class="social-btn" aria-label="Email"><i class="fa-solid fa-envelope"></i></a>
          <a href="https://scholar.google.com/citations?user=rCvK7tcAAAAJ" target="_blank" rel="noopener" class="social-btn" aria-label="Google Scholar"><i class="fa-solid fa-graduation-cap"></i></a>
          <a href="https://github.com/chehx" target="_blank" rel="noopener" class="social-btn" aria-label="GitHub"><i class="fa-brands fa-github"></i></a>
          <a href="https://www.linkedin.com/in/hche" target="_blank" rel="noopener" class="social-btn" aria-label="LinkedIn"><i class="fa-brands fa-linkedin-in"></i></a>
        </div>

        <ul class="nav-menu">
          <li><a href="#about" class="nav-link active">About</a></li>
          <li><a href="#news" class="nav-link">News</a></li>
          <li><a href="#publications" class="nav-link">Publications</a></li>
          <li><a href="#awards" class="nav-link">Honors & Awards</a></li>
          <li><a href="#service" class="nav-link">Service</a></li>
          <li><a href="#teaching" class="nav-link">Teaching</a></li>
          <li><a href="#grants" class="nav-link">Grants</a></li>
          <li><a href="#internships" class="nav-link">Internships</a></li>
        </ul>
      </div>
    </aside>

    <!-- RIGHT CONTENT -->
    <main>
      <nav class="mobile-nav" aria-label="Page sections">
        <a href="#about" class="nav-link">About</a>
        <a href="#news" class="nav-link">News</a>
        <a href="#publications" class="nav-link">Publications</a>
        <a href="#awards" class="nav-link">Honors &amp; Awards</a>
        <a href="#service" class="nav-link">Service</a>
        <a href="#teaching" class="nav-link">Teaching</a>
        <a href="#grants" class="nav-link">Grants</a>
        <a href="#internships" class="nav-link">Internships</a>
      </nav>
      
      <!-- ABOUT -->
      <section id="about">
        <h2>About Me</h2>
        <p>
          I am currently a Principal Researcher responsible for visual generation and world model, at <b>AI Lab, Huawei Hong Kong Research Center</b>.
          Previously, I obtained my Ph.D. in Computer Science and Engineering at 
          <a href="https://hkust.edu.hk/" target="_blank" rel="noopener">The Hong Kong University of Science and Technology (HKUST)</a>.
          Prior to joining HKUST, I obtained a Bachelor's degree from 
          <a href="https://en.nwpu.edu.cn/" target="_blank" rel="noopener">Northwestern Polytechnical University (NWPU)</a> with honor.
        </p>
        <p>
          My primary research interests lie in <b>world model</b>, <b>multi-modal generative model</b>, and <b>AI agents</b>.
        </p>
        <p class="mt-12">
          Our team is dedicated to advancing research in Multimodal Generative Models and AI Agents. We are actively seeking experienced full-time researchers and interns to join us. If you're interested, feel free to reach out via email.
        </p>
      </section>

      <!-- NEWS (Scrollable) -->
      <section id="news">
        <h2>ğŸ”¥ News</h2>
        <div class="news-card">
          <div class="news-scroll-area">
            <div class="news-item"><span class="news-date">2026.02</span><span>ğŸ‰ğŸ‰ We release <b>Capybara</b>, the first unified visual creation model! </span></div>
            <div class="news-item"><span class="news-date">2026.02</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>CVPR 2026</b>!</span></div>
            <div class="news-item"><span class="news-date">2025.11</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>AAAI 2026</b>!</span></div>
            <div class="news-item"><span class="news-date">2025.07</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>IEEE TMI</b>!</span></div>
            <div class="news-item"><span class="news-date">2025.06</span><span>ğŸ‰ğŸ‰ Two papers got accepted by <b>IEEE TMI</b>!</span></div>
            <div class="news-item"><span class="news-date">2025.05</span><span>ğŸ‰ğŸ‰ I successfully passed my PhD thesis defense, thanks to my advisor and all the committee members!</span></div>
            <div class="news-item"><span class="news-date">2025.03</span><span>ğŸ‰ğŸ‰ I have started my Student Researchership at Intelligent Creation, ByteDance.</span></div>
            <div class="news-item"><span class="news-date">2025.01</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>ICLR 2025</b>, see you in Singapore!</span></div>
            <div class="news-item"><span class="news-date">2024.12</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>IEEE JBHI</b>!</span></div>
            <div class="news-item"><span class="news-date">2024.11</span><span>ğŸ‰ğŸ‰ I have started researching interactive video generation at Kling Team, Kuaishou!</span></div>
            <div class="news-item"><span class="news-date">2024.08</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>IEEE Transactions on Image Processing</b> and will be published soon!</span></div>
            <div class="news-item"><span class="news-date">2024.05</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>MICCAI 2024</b>, see you in Marrakesh!</span></div>
            <div class="news-item"><span class="news-date">2024.05</span><span>ğŸ‰ğŸ‰ I have started my Student Researchership at Tencent!</span></div>
            <div class="news-item"><span class="news-date">2024.01</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>Patterns</b> and will be published soon!</span></div>
            <div class="news-item"><span class="news-date">2023.12</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>AAAI 2024</b>, see you in Vancouver!</span></div>
            <div class="news-item"><span class="news-date">2023.11</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>Nature Medicine</b> and will be published soon!</span></div>
            <div class="news-item"><span class="news-date">2023.06</span><span>ğŸ‰ğŸ‰ I successfully passed my PhD qualifying examination!</span></div>
            <div class="news-item"><span class="news-date">2023.05</span><span>ğŸ‰ğŸ‰ Our two papers got accepted by <b>MICCAI 2023</b> (one early accepted), see you in Vancouver!</span></div>
            <div class="news-item"><span class="news-date">2023.03</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>CVPR 2023</b>, see you in Vancouver!</span></div>
            <div class="news-item"><span class="news-date">2023.02</span><span>ğŸ‰ğŸ‰ Our paper was shortlisted by <a href="https://cse.hkust.edu.hk/event/RTF2023/" target="_blank" rel="noopener">CSE Research and Technology Forum</a>!</span></div>
            <div class="news-item"><span class="news-date">2022.07</span><span>ğŸ‰ğŸ‰ Glad to be selected for <a href="https://conferences.miccai.org/2022/en/MICCAI-2022-STUDENT-TRAVEL-AWARDS.html" target="_blank" rel="noopener">MICCAI Travel Award</a>!</span></div>
            <div class="news-item"><span class="news-date">2022.05</span><span>ğŸ‰ğŸ‰ Our paper got early accepted by <b>MICCAI 2022</b>, see you in Singapore!</span></div>
            <div class="news-item"><span class="news-date">2022.03</span><span>ğŸ‰ğŸ‰ Honored to be the person in charge of 2022/23 <a href="https://www.itf.gov.hk/en/project-search/project-profile/index.html?ReferenceNo=TSSSU/HKUST/22/03" target="_blank" rel="noopener">TSSSU</a>. Thank you, Innovation and Technology Commission!</span></div>
            <div class="news-item"><span class="news-date">2021.10</span><span>ğŸ‰ğŸ‰ Our paper got accepted by <b>IEEE JSAC</b>!</span></div>
          </div>
        </div>
      </section>

      <!-- PUBLICATIONS (Split Categories & Landscape Images) -->
      <section id="publications">
        <h2>ğŸ“ Selected Publications <span class="pub-full-list"></span></h2>
        <p class="section-note"> * denotes co-first authorship, â€  denotes project leadership,  âœ‰ï¸ denotes correspondence author.</p>

        <div class="topic-title">VISUAL GENERATION &amp; WORLD MODELS</div>

        <article class="paper-card">
          <div class="paper-img">
            <img loading="lazy" src="https://github.com/xgen-universe/Capybara/raw/main/assets/misc/logo.png" alt="Capybara">
          </div>
          <div class="paper-content">
            <div class="paper-title">Capybara: The First Unified Visual Creation Model</div>
            <div class="paper-authors">Zhefan Rao, <b>Haoxuan Cheâ€ âœ‰ï¸</b>, Ziwen Hu, Bin Zou, Yaofang Liu, Xuanhua He, Chong-Hou Choi, Yuyang He, Haoyu Chen, Jingran Su, Yanheng Li, Meng Chu, Chenyang Lei, Guanhua Zhao, Zhaoqing Li, Xichen Zhang, Anping Li, Lin Liu, Dandan Tu, Rui Liu</div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">Technical Report</span>
              <span class="paper-meta-item paper-year">2026</span>
            </div>
            <div class="btn-row">
              <a href="https://github.com/xgen-universe/Capybara" class="btn-pill" data-github="xgen-universe/Capybara"><i class="fa-brands fa-github"></i> Github <span class="badge-count gh-stars"></span></a>
              <a href="https://huggingface.co/xgen-universe/Capybara" class="btn-pill" data-hf="xgen-universe/Capybara"><i class="fa-solid fa-database"></i> HF <span class="badge-count hf-likes"></span></a>
              <a href="https://github.com/xgen-universe/Capybara/blob/main/assets/docs/tech_report.pdf" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> Report</a>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-img">
            <img loading="lazy" src="https://visiondirector.github.io/static/images/demo_horse-1.png" alt="VisionDirector">
          </div>
          <div class="paper-content">
            <div class="paper-title">VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis</div>
            <div class="paper-authors">Meng Chu, Senqiao Yang, <b>Haoxuan Cheâ€ âœ‰ï¸</b>, Suiyun Zhang, Xichen Zhang, Shaozuo Yu, Haokun Gui, Zhefan Rao, Dandan Tu, Rui Liu, Jiaya Jia.</div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">CVPR</span>
              <span class="paper-meta-item paper-year">2026</span>
              <span class="paper-meta-item paper-rate">AR: 25.42%</span>
            </div>
            <div class="btn-row">
              <a href="https://visiondirector.github.io/" class="btn-pill"><i class="fa-solid fa-globe"></i> Project</a>
              <a href="https://arxiv.org/abs/2512.19243" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> PDF</a>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-img">
            <video src="https://github.com/3A2077/3A2077.github.io/raw/main/videos/OGameGen_Func.mp4" muted autoplay loop playsinline></video>
          </div>
          <div class="paper-content">
            <div class="paper-title">GameGen-X: Interactive Open-World Game Video Generation</div>
            <div class="paper-authors"><b>Haoxuan Che</b>, Xuanhua He, Quande Liu, Cheng Jin, Hao Chen</div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">ICLR</span>
              <span class="paper-meta-item paper-year">2025</span>
              <span class="paper-meta-item paper-rate">AR: 32.08%</span>
            </div>
            <div class="btn-row">
              <a href="https://gamegen-x.github.io/" class="btn-pill"><i class="fa-solid fa-globe"></i> Project</a>
              <a href="https://arxiv.org/abs/2411.00769" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> PDF</a>
              <a href="https://github.com/GameGen-X/GameGen-X" class="btn-pill" data-github="GameGen-X/GameGen-X"><i class="fa-brands fa-github"></i> Code <span class="badge-count gh-stars"></span></a>
              <a href="https://36kr.com/p/2949643749958018" target="_blank" rel="noopener" class="btn-pill"><i class="fa-solid fa-newspaper"></i> é‡å­ä½</a>
              <a href="https://news.qq.com/rain/a/20241106A04FV600" target="_blank" rel="noopener" class="btn-pill"><i class="fa-solid fa-newspaper"></i> æœºå™¨ä¹‹å¿ƒ</a>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-img">
            <img loading="lazy" src="https://yujiwen.github.io/images/paper/igv_survey.jpg" alt="igensurvey">
          </div>
          <div class="paper-content">
            <div class="paper-title">A Survey of Interactive Generative Video</div>
            <div class="paper-authors">Jiwen Yu, Yiran Qin, <b>Haoxuan Che*</b>, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, Xihui Liu</div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">Pre-print</span>
              <span class="paper-meta-item paper-year">2025</span>
            </div>
            <div class="btn-row">
              <a href="https://arxiv.org/pdf/2504.21853" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> PDF</a>
            </div>
          </div>  
        </article>

        <article class="paper-card">
          <div class="paper-img">
            <img loading="lazy" src="images/publications/2026/mmmamba.png" alt="mmmamba">
          </div>
          <div class="paper-content">
            <div class="paper-title">MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement</div>
            <div class="paper-authors">Yingying Wang, Xuanhua He, Chen Wu, Jialing Huang, Suiyun Zhang, Rui Liu, Xinghao Ding, <b>Haoxuan Cheâœ‰ï¸</b></div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">AAAI</span>
              <span class="paper-meta-item paper-year">2026</span>
              <span class="paper-meta-item paper-rate">AR: 17.6%</span>
            </div>
            <div class="btn-row">
              <a href="https://arxiv.org/abs/2512.15261" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> PDF</a>
              <a href="https://github.com/Gracewangyy/MMMamba" class="btn-pill" data-github="Gracewangyy/MMMamba"><i class="fa-brands fa-github"></i> Code <span class="badge-count gh-stars"></span></a>
            </div>
          </div>
        </article>

        <div class="topic-title">MODEL GENERALIZATION &amp; DATA SHIFT</div>

        <article class="paper-card">
          <div class="paper-img"><img loading="lazy" src="https://arxiv.org/html/2506.17562v2/extracted/6633343/che1.png" alt="LLM-driven MRG"></div>
          <div class="paper-content">
            <div class="paper-title">LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning</div>
            <div class="paper-authors"><b>Haoxuan Che</b>, Haibo Jin, Zhengrui Guo, Yi Lin, Cheng Jin, Hao Chen</div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">IEEE TMI</span>
              <span class="paper-meta-item paper-year">2025</span>
              <span class="paper-meta-item paper-if">IF: 9.8</span>
            </div>
            <div class="btn-row">
              <a href="https://arxiv.org/pdf/2506.17562" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> PDF</a>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-img"><img loading="lazy" src="https://arxiv.org/html/2501.13967v2/x1.png" alt="FedDAG"></div>
          <div class="paper-content">
            <div class="paper-title">FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis</div>
            <div class="paper-authors"><b>Haoxuan Che</b>, Yifei Wu, Haibo Jin, Yong Xia, Hao Chen</div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">IEEE TMI</span>
              <span class="paper-meta-item paper-year">2025</span>
              <span class="paper-meta-item paper-if">IF: 9.8</span>
            </div>
            <div class="btn-row">
              <a href="https://arxiv.org/abs/2501.13967" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> PDF</a>
              <a href="https://github.com/chehx/DGDR" class="btn-pill" data-github="chehx/DGDR"><i class="fa-brands fa-github"></i> Code <span class="badge-count gh-stars"></span></a>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-img"><img loading="lazy" src="https://arxiv.org/html/2404.04556v2/x1.png" alt="STLD"></div>
          <div class="paper-content">
            <div class="paper-title">Rethinking Self-training for Semi-supervised Landmark Detection: A Selection-free Approach</div>
            <div class="paper-authors">Haibo Jin, <b>Haoxuan Che*</b>, Hao Chen</div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">IEEE TIP</span>
              <span class="paper-meta-item paper-year">2024</span>
              <span class="paper-meta-item paper-if">IF: 10.8</span>
            </div>
            <div class="btn-row">
              <a href="https://arxiv.org/abs/2404.04556" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> PDF</a>
              <a href="https://github.com/jhb86253817/STLD" class="btn-pill" data-github="jhb86253817/STLD"><i class="fa-brands fa-github"></i> Code <span class="badge-count gh-stars"></span></a>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-img"><img loading="lazy" src="https://arxiv.org/html/2308.12604v2/x2.png" alt="PromptMRG"></div>
          <div class="paper-content">
            <div class="paper-title">PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation</div>
            <div class="paper-authors">Haibo Jin, <b>Haoxuan Che</b>, Yi Lin, Hao Chen</div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">AAAI</span>
              <span class="paper-meta-item paper-year">2024</span>
              <span class="paper-meta-item paper-rate">AR: 23.75%</span>
            </div>
            <div class="btn-row">
              <a href="https://arxiv.org/abs/2308.12604" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> PDF</a>
              <a href="https://github.com/jhb86253817/PromptMRG" class="btn-pill" data-github="jhb86253817/PromptMRG"><i class="fa-brands fa-github"></i> Code <span class="badge-count gh-stars"></span></a>
            </div>
          </div>
        </article>

        <article class="paper-card">
          <div class="paper-img"><img loading="lazy" src="https://figures.semanticscholar.org/a47f97b0de1b33604a73a350b931b1468e75a352/3-Figure2-1.png" alt="MKCNet"></div>
          <div class="paper-content">
            <div class="paper-title">Image Quality-aware Diagnosis via Meta-knowledge Co-embedding</div>
            <div class="paper-authors"><b>Haoxuan Che</b>, Siyu Chen, Hao Chen</div>
            <div class="paper-meta-row">
              <span class="paper-venue highlight">CVPR</span>
              <span class="paper-meta-item paper-year">2023</span>
              <span class="paper-meta-item paper-rate">AR: 25.78%</span>
            </div>
            <div class="btn-row">
              <a href="https://arxiv.org/abs/2303.15038" class="btn-pill"><i class="fa-regular fa-file-pdf"></i> PDF</a>
              <a href="https://github.com/chehx/MKCNet" class="btn-pill" data-github="chehx/MKCNet"><i class="fa-brands fa-github"></i> Code <span class="badge-count gh-stars"></span></a>
            </div>
          </div>
        </article>

      </section>

      <!-- AWARDS -->
      <section id="awards">
        <h2>ğŸ– Selected Honors and Awards</h2>
        <div class="info-card">
          <ul class="awards-list">
            <li>The First Prize and Application Innovation Award in the ASC17 Student Supercomputer Challenge, 1/230 (2017)</li>
            <li>The Excellent Paper Award for the National College Students Mathematics Modeling Competition, 12/33062 (2017)</li>
            <li>The First Prize in the National College Students Mathematics Modeling Competition, Top 0.88% (2017)</li>
            <li>The China National Scholarship for two years, Top 2% (2017/2018)</li>
            <li>The Honor of Outstanding Graduate at Northwestern Polytechnical University, Top 5% (2019)</li>
          </ul>
        </div>
      </section>

      <!-- PROFESSIONAL SERVICE -->
      <section id="service">
        <h2>ğŸ’» Professional Service</h2>
        <div class="info-card">
          <p class="section-label">Regular Conference Program Committee Member</p>
          <ul class="info-list">
            <li>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</li>
            <li>International Conference on Computer Vision (ICCV)</li>
            <li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</li>
            <li>AAAI Conference on Artificial Intelligence (AAAI)</li>
          </ul>
          <p class="section-label">Regular Journal Reviewer</p>
          <ul class="info-list">
            <li>Information Fusion</li>
            <li>IEEE Transactions on Medical Imaging (TMI)</li>
            <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
            <li>IEEE Transactions on Image Processing (TIP)</li>
            <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
          </ul>
          <p class="section-label">Competition and Workshop Organization</p>
          <ul class="info-list">
            <li><a href="https://drac22.grand-challenge.org/" target="_blank" rel="noopener">DRAC 2022: Diabetic Retinopathy Analysis Challenge</a> (within MICCAI 22)</li>
          </ul>
        </div>
      </section>

      <!-- TEACHING -->
      <section id="teaching">
        <h2>ğŸ“š Teaching Experience</h2>
        <div class="info-card">
          <p class="section-label">Course</p>
          <ul class="info-list info-list--spaced">
            <li>COMP 4451 Game Programming, Teaching Assistant, 2021-2022 Spring Semester.</li>
            <li>COMP 4421 Image Processing, Teaching Assistant, 2022-2023 Fall Semester.</li>
            <li>COMP 3321 Fundamentals of Artificial Intelligence, Teaching Assistant, 2023-2024 Fall Semester.</li>
            <li>COMP 1001 Exploring Multimedia and Internet Computing, Teaching Assistant, 2024-2025 Spring Semester.</li>
          </ul>
          <p class="section-label">Mentee</p>
          <ul class="info-list info-list--spaced">
            <li>2021.11 - 2023.01, Siyu Chen (UG Student @ HKUST, next: Ph.D. Student in HKUST)</li>
            <li>2023.06 - 2024.02, Hoi-Tin Wong (UG Student @ HKUST)</li>
            <li>2023.07 - 2024.02, Lingqi Zeng (UG Student @ HKUST)</li>
            <li>2022.06 - 2024.05, Yuhan Cheng (UG Student @ HKUST, next: M.Ph. Student in HKUST(GZ))</li>
            <li>2023.08 - 2025.02, Kai Syun Hou (M.Ph. Student in HKUST)</li>
            <li>2023.02 - 2025.06, Yifei Wu (M.S. Student @ NWPU)</li>
            <li>2025.05 - present, Jingran Su (Ph.D. Candidate @ PolyU)</li>
            <li>2025.05 - present, Zhaoqing Li (Ph.D. Candidate @ CUHK)</li>
            <li>2025.06 - present, Zhefan Rao (Ph.D. Candidate @ HKUST)</li>
            <li>2025.06 - present, Meng Chu (Ph.D. Candidate @ HKUST)</li>
          </ul>
        </div>
      </section>

      <!-- GRANTS -->
      <section id="grants">
        <h2>ğŸ’¬ Grants</h2>
        <div class="info-card">
          <p class="grants-text">
            2022.03 - 2024.03, <a href="https://www.itf.gov.hk/en/project-search/project-profile/index.html?ReferenceNo=TSSSU/HKUST/22/03" target="_blank" rel="noopener">Technology Start-up Support Scheme for Universities (TSSSU)</a>, The Innovation and Technology Commission (ITC) of the Hong Kong SAR.
          </p>
        </div>
      </section>

      <!-- INTERNSHIPS -->
      <section id="internships">
        <h2>ğŸ’¼ Internships</h2>
        <div class="info-card internships-card">
          <div class="internship-item">
            <div class="internship-logo">
              <img src="images/misc/Tencent.webp" alt="Tencent">
            </div>
            <div>
              <div class="internship-title">Student Researcher, Tencent, China</div>
              <div class="internship-time">2024.05 - 2024.10</div>
            </div>
          </div>
          <div class="internship-item">
            <div class="internship-logo">
              <img src="images/misc/kuaishou.jpg" alt="Kuaishou">
            </div>
            <div>
              <div class="internship-title">Student Researcher, Kling Team, Kuaishou (Kwai), China</div>
              <div class="internship-time">2024.11 - 2025.03</div>
            </div>
          </div>
          <div class="internship-item">
            <div class="internship-logo">
              <img src="images/misc/bytedance.png" alt="ByteDance">
            </div>
            <div>
              <div class="internship-title">Student Researcher, Intelligent Creation, ByteDance, China</div>
              <div class="internship-time">2025.03 - 2025.06</div>
            </div>
          </div>
        </div>
      </section>

      <footer class="site-footer">
        <p class="footer-copy">Â© 2025 Haoxuan Cheï¼ˆè½¦æ˜Šè½©ï¼‰.</p>
        <div class="footer-map">
          <script type="text/javascript" id="clustrmaps" src="https://clustrmaps.com/map_v2.js?d=PWsK4o-74_lvDUbcOuvi8hdLi2fjqiuEfiudgJdrt5k&cl=ffffff&w=a"></script>
        </div>
      </footer>

    </main>
  </div>

  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <script src="js/main.js"></script>
</body>

</html>




